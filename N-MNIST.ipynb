{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc92b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transformed array is in shape [Time-Step, Channel, Height, Width] --> (1, 2, 34, 34)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m cnn \u001b[38;5;241m=\u001b[39m \u001b[43mcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m cnn_train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(cnn_train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, num_workers\u001b[38;5;241m=\u001b[39mnum_workers, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shuffle\u001b[38;5;241m=\u001b[39mshuffle)\n\u001b[1;32m     67\u001b[0m cnn_test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(cnn_test_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, num_workers\u001b[38;5;241m=\u001b[39mnum_workers, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shuffle\u001b[38;5;241m=\u001b[39mshuffle)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# 导入必需的库\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "import samna\n",
    "from tonic.datasets import NMNIST\n",
    "from tonic.transforms import ToFrame\n",
    "from sinabs.from_torch import from_model\n",
    "from sinabs.backend.dynapcnn import DynapcnnNetwork\n",
    "from collections import Counter\n",
    "\n",
    "# 数据准备\n",
    "root_dir = \"./NMNIST\"\n",
    "try:\n",
    "    from tonic.datasets.nmnist import NMNIST\n",
    "except ImportError:\n",
    "    !pip install tonic\n",
    "    from tonic.datasets.nmnist import NMNIST\n",
    "\n",
    "# 下载 N-MNIST 数据集\n",
    "_ = NMNIST(save_to=root_dir, train=True)\n",
    "_ = NMNIST(save_to=root_dir, train=False)\n",
    "\n",
    "# 使用 Tonic 转换事件数据为帧\n",
    "to_frame = ToFrame(sensor_size=NMNIST.sensor_size, n_time_bins=1)\n",
    "cnn_train_dataset = NMNIST(save_to=root_dir, train=True, transform=to_frame)\n",
    "cnn_test_dataset = NMNIST(save_to=root_dir, train=False, transform=to_frame)\n",
    "\n",
    "# 打印一下数据的形状\n",
    "sample_data, label = cnn_train_dataset[0]\n",
    "print(f\"The transformed array is in shape [Time-Step, Channel, Height, Width] --> {sample_data.shape}\")\n",
    "\n",
    "# 定义 CNN 模型\n",
    "cnn = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=2, out_channels=8, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=(1, 1), stride=(2, 2), bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 4 * 4, 10, bias=False),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "# 初始化模型权重\n",
    "for layer in cnn.modules():\n",
    "    if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.xavier_normal_(layer.weight.data)\n",
    "\n",
    "# 训练和验证 CNN\n",
    "epochs = 3\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "shuffle = True\n",
    "\n",
    "cnn = cnn.to(device=device)\n",
    "\n",
    "cnn_train_dataloader = DataLoader(cnn_train_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=shuffle)\n",
    "cnn_test_dataloader = DataLoader(cnn_test_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=shuffle)\n",
    "\n",
    "optimizer = SGD(params=cnn.parameters(), lr=lr)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "for e in range(epochs):\n",
    "    # Train the model\n",
    "    train_p_bar = tqdm(cnn_train_dataloader)\n",
    "    for data, label in train_p_bar:\n",
    "        data = data.squeeze(dim=1).to(dtype=torch.float, device=device)\n",
    "        label = label.to(dtype=torch.long, device=device)\n",
    "        optimizer.zero_grad()\n",
    "        output = cnn(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_p_bar.set_description(f\"Epoch {e} - Training Loss: {round(loss.item(), 4)}\")\n",
    "\n",
    "    # Validate the model\n",
    "    correct_predictions = []\n",
    "    with torch.no_grad():\n",
    "        test_p_bar = tqdm(cnn_test_dataloader)\n",
    "        for data, label in test_p_bar:\n",
    "            data = data.squeeze(dim=1).to(dtype=torch.float, device=device)\n",
    "            label = label.to(dtype=torch.long, device=device)\n",
    "            output = cnn(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct_predictions.append(pred.eq(label.view_as(pred)))\n",
    "            test_p_bar.set_description(f\"Epoch {e} - Testing Model...\")\n",
    "    \n",
    "        correct_predictions = torch.cat(correct_predictions)\n",
    "        print(f\"Epoch {e} - accuracy: {correct_predictions.sum().item()/(len(correct_predictions))*100}%\")\n",
    "\n",
    "# CNN 转 SNN\n",
    "snn_convert = from_model(model=cnn, input_shape=(2, 34, 34), batch_size=batch_size).spiking_model\n",
    "\n",
    "# 转换后的 SNN 测试\n",
    "n_time_steps = 100\n",
    "to_raster = ToFrame(sensor_size=NMNIST.sensor_size, n_time_bins=n_time_steps)\n",
    "snn_test_dataset = NMNIST(save_to=root_dir, train=False, transform=to_raster)\n",
    "snn_test_dataloader = DataLoader(snn_test_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=False)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "snn_convert = snn_convert.to(device)\n",
    "\n",
    "correct_predictions = []\n",
    "with torch.no_grad():\n",
    "    test_p_bar = tqdm(snn_test_dataloader)\n",
    "    for data, label in test_p_bar:\n",
    "        data = data.reshape(-1, 2, 34, 34).to(dtype=torch.float, device=device)\n",
    "        label = label.to(dtype=torch.long, device=device)\n",
    "        output = snn_convert(data)\n",
    "        output = output.reshape(batch_size, n_time_steps, -1)\n",
    "        output = output.sum(dim=1)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct_predictions.append(pred.eq(label.view_as(pred)))\n",
    "        test_p_bar.set_description(f\"Testing SNN Model...\")\n",
    "\n",
    "    correct_predictions = torch.cat(correct_predictions)\n",
    "    print(f\"accuracy of converted SNN: {correct_predictions.sum().item()/(len(correct_predictions))*100}%\")\n",
    "\n",
    "# 部署 SNN 到 Devkit\n",
    "cpu_snn = snn_convert.to(device=\"cpu\")\n",
    "devices = samna.device.get_unopened_devices()\n",
    "if len(devices) == 0:\n",
    "    raise Exception(\"No devices found\")\n",
    "my_board = samna.device.open_device(devices[0])\n",
    "\n",
    "# 使用设备信息部署到 DevKit\n",
    "devkit_name = my_board.devkit_name\n",
    "dynapcnn = DynapcnnNetwork(snn=cpu_snn, input_shape=(2, 34, 34), discretize=True, dvs_input=False)\n",
    "dynapcnn.to(device=devkit_name, chip_layers_ordering=\"auto\")\n",
    "\n",
    "print(f\"The SNN is deployed on the core: {dynapcnn.chip_layers_ordering}\")\n",
    "\n",
    "# 推理\n",
    "snn_test_dataset = NMNIST(save_to=root_dir, train=False)\n",
    "subset_indices = list(range(0, len(snn_test_dataset), 100))\n",
    "snn_test_dataset = Subset(snn_test_dataset, subset_indices)\n",
    "\n",
    "correct_samples = 0\n",
    "test_samples = 0\n",
    "\n",
    "for events, label in tqdm(snn_test_dataset):\n",
    "    samna_event_stream = []\n",
    "    for ev in events:\n",
    "        spk = samna.speck2f.event.Spike()\n",
    "        spk.x = ev['x']\n",
    "        spk.y = ev['y']\n",
    "        spk.timestamp = ev['t'] - events['t'][0]\n",
    "        spk.feature = ev['p']\n",
    "        spk.layer = 0\n",
    "        samna_event_stream.append(spk)\n",
    "\n",
    "    output_events = dynapcnn(samna_event_stream)\n",
    "    neuron_index = [each.feature for each in output_events]\n",
    "    if len(neuron_index) != 0:\n",
    "        frequent_counter = Counter(neuron_index)\n",
    "        prediction = frequent_counter.most_common(1)[0][0]\n",
    "    else:\n",
    "        prediction = -1\n",
    "\n",
    "    if prediction == label:\n",
    "        correct_samples += 1\n",
    "    test_samples += 1\n",
    "\n",
    "print(f\"On chip inference accuracy: {correct_samples / test_samples}\")\n",
    "\n",
    "# 可视化\n",
    "from sinabs.backend.dynapcnn.dynapcnn_visualizer import DynapcnnVisualizer\n",
    "visualizer = DynapcnnVisualizer(window_scale=(4, 8), dvs_shape=(34, 34), spike_collection_interval=50)\n",
    "visualizer.connect(dynapcnn)\n",
    "\n",
    "print(\"Visualizer setup complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
