{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bc92b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transformed array is in shape [Time-Step, Channel, Height, Width] --> (1, 2, 34, 34)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training Loss: 1.3631: 100%|██████████| 15000/15000 [02:47<00:00, 89.34it/s] \n",
      "Epoch 0 - Testing Model...: 100%|██████████| 2500/2500 [00:43<00:00, 56.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - accuracy: 48.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 1.2505: 100%|██████████| 15000/15000 [02:47<00:00, 89.34it/s] \n",
      "Epoch 1 - Testing Model...: 100%|██████████| 2500/2500 [00:43<00:00, 57.62it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - accuracy: 48.949999999999996%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.5779: 100%|██████████| 15000/15000 [02:39<00:00, 93.80it/s] \n",
      "Epoch 2 - Testing Model...: 100%|██████████| 2500/2500 [00:43<00:00, 57.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - accuracy: 57.410000000000004%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing SNN Model...: 100%|██████████| 2500/2500 [06:46<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of converted SNN: 55.900000000000006%\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'samna.speck2fBoards.Speck2fDevKit' object has no attribute 'devkit_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 136\u001b[0m\n\u001b[1;32m    133\u001b[0m my_board \u001b[38;5;241m=\u001b[39m samna\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mopen_device(devices[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# 使用设备信息部署到 DevKit\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m devkit_name \u001b[38;5;241m=\u001b[39m \u001b[43mmy_board\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevkit_name\u001b[49m\n\u001b[1;32m    137\u001b[0m dynapcnn \u001b[38;5;241m=\u001b[39m DynapcnnNetwork(snn\u001b[38;5;241m=\u001b[39mcpu_snn, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m34\u001b[39m, \u001b[38;5;241m34\u001b[39m), discretize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dvs_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    138\u001b[0m dynapcnn\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevkit_name, chip_layers_ordering\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'samna.speck2fBoards.Speck2fDevKit' object has no attribute 'devkit_name'"
     ]
    }
   ],
   "source": [
    "# 导入必需的库\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "import samna\n",
    "from tonic.datasets import NMNIST\n",
    "from tonic.transforms import ToFrame\n",
    "from sinabs.from_torch import from_model\n",
    "from sinabs.backend.dynapcnn import DynapcnnNetwork\n",
    "from collections import Counter\n",
    "\n",
    "# 数据准备\n",
    "root_dir = \"./NMNIST\"\n",
    "try:\n",
    "    from tonic.datasets.nmnist import NMNIST\n",
    "except ImportError:\n",
    "    !pip install tonic\n",
    "    from tonic.datasets.nmnist import NMNIST\n",
    "\n",
    "# 下载 N-MNIST 数据集\n",
    "_ = NMNIST(save_to=root_dir, train=True)\n",
    "_ = NMNIST(save_to=root_dir, train=False)\n",
    "\n",
    "# 使用 Tonic 转换事件数据为帧\n",
    "to_frame = ToFrame(sensor_size=NMNIST.sensor_size, n_time_bins=1)\n",
    "cnn_train_dataset = NMNIST(save_to=root_dir, train=True, transform=to_frame)\n",
    "cnn_test_dataset = NMNIST(save_to=root_dir, train=False, transform=to_frame)\n",
    "\n",
    "# 打印一下数据的形状\n",
    "sample_data, label = cnn_train_dataset[0]\n",
    "print(f\"The transformed array is in shape [Time-Step, Channel, Height, Width] --> {sample_data.shape}\")\n",
    "\n",
    "# 定义 CNN 模型\n",
    "cnn = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=2, out_channels=8, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=(1, 1), stride=(2, 2), bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 4 * 4, 10, bias=False),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "# 初始化模型权重\n",
    "for layer in cnn.modules():\n",
    "    if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.xavier_normal_(layer.weight.data)\n",
    "\n",
    "# 训练和验证 CNN\n",
    "epochs = 3\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "shuffle = True\n",
    "\n",
    "cnn = cnn.to(device=device)\n",
    "\n",
    "cnn_train_dataloader = DataLoader(cnn_train_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=shuffle)\n",
    "cnn_test_dataloader = DataLoader(cnn_test_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=shuffle)\n",
    "\n",
    "optimizer = SGD(params=cnn.parameters(), lr=lr)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "for e in range(epochs):\n",
    "    # Train the model\n",
    "    train_p_bar = tqdm(cnn_train_dataloader)\n",
    "    for data, label in train_p_bar:\n",
    "        data = data.squeeze(dim=1).to(dtype=torch.float, device=device)\n",
    "        label = label.to(dtype=torch.long, device=device)\n",
    "        optimizer.zero_grad()\n",
    "        output = cnn(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_p_bar.set_description(f\"Epoch {e} - Training Loss: {round(loss.item(), 4)}\")\n",
    "\n",
    "    # Validate the model\n",
    "    correct_predictions = []\n",
    "    with torch.no_grad():\n",
    "        test_p_bar = tqdm(cnn_test_dataloader)\n",
    "        for data, label in test_p_bar:\n",
    "            data = data.squeeze(dim=1).to(dtype=torch.float, device=device)\n",
    "            label = label.to(dtype=torch.long, device=device)\n",
    "            output = cnn(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct_predictions.append(pred.eq(label.view_as(pred)))\n",
    "            test_p_bar.set_description(f\"Epoch {e} - Testing Model...\")\n",
    "    \n",
    "        correct_predictions = torch.cat(correct_predictions)\n",
    "        print(f\"Epoch {e} - accuracy: {correct_predictions.sum().item()/(len(correct_predictions))*100}%\")\n",
    "\n",
    "# CNN 转 SNN\n",
    "snn_convert = from_model(model=cnn, input_shape=(2, 34, 34), batch_size=batch_size).spiking_model\n",
    "\n",
    "# 转换后的 SNN 测试\n",
    "n_time_steps = 100\n",
    "to_raster = ToFrame(sensor_size=NMNIST.sensor_size, n_time_bins=n_time_steps)\n",
    "snn_test_dataset = NMNIST(save_to=root_dir, train=False, transform=to_raster)\n",
    "snn_test_dataloader = DataLoader(snn_test_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=False)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "snn_convert = snn_convert.to(device)\n",
    "\n",
    "correct_predictions = []\n",
    "with torch.no_grad():\n",
    "    test_p_bar = tqdm(snn_test_dataloader)\n",
    "    for data, label in test_p_bar:\n",
    "        data = data.reshape(-1, 2, 34, 34).to(dtype=torch.float, device=device)\n",
    "        label = label.to(dtype=torch.long, device=device)\n",
    "        output = snn_convert(data)\n",
    "        output = output.reshape(batch_size, n_time_steps, -1)\n",
    "        output = output.sum(dim=1)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct_predictions.append(pred.eq(label.view_as(pred)))\n",
    "        test_p_bar.set_description(f\"Testing SNN Model...\")\n",
    "\n",
    "    correct_predictions = torch.cat(correct_predictions)\n",
    "    print(f\"accuracy of converted SNN: {correct_predictions.sum().item()/(len(correct_predictions))*100}%\")\n",
    "\n",
    "# 部署 SNN 到 Devkit\n",
    "cpu_snn = snn_convert.to(device=\"cpu\")\n",
    "devices = samna.device.get_unopened_devices()\n",
    "if len(devices) == 0:\n",
    "    raise Exception(\"No devices found\")\n",
    "my_board = samna.device.open_device(devices[0])\n",
    "\n",
    "# 使用设备信息部署到 DevKit\n",
    "devkit_name = my_board.devkit_name\n",
    "dynapcnn = DynapcnnNetwork(snn=cpu_snn, input_shape=(2, 34, 34), discretize=True, dvs_input=False)\n",
    "dynapcnn.to(device=devkit_name, chip_layers_ordering=\"auto\")\n",
    "\n",
    "print(f\"The SNN is deployed on the core: {dynapcnn.chip_layers_ordering}\")\n",
    "\n",
    "# 推理\n",
    "snn_test_dataset = NMNIST(save_to=root_dir, train=False)\n",
    "subset_indices = list(range(0, len(snn_test_dataset), 100))\n",
    "snn_test_dataset = Subset(snn_test_dataset, subset_indices)\n",
    "\n",
    "correct_samples = 0\n",
    "test_samples = 0\n",
    "\n",
    "for events, label in tqdm(snn_test_dataset):\n",
    "    samna_event_stream = []\n",
    "    for ev in events:\n",
    "        spk = samna.speck2f.event.Spike()\n",
    "        spk.x = ev['x']\n",
    "        spk.y = ev['y']\n",
    "        spk.timestamp = ev['t'] - events['t'][0]\n",
    "        spk.feature = ev['p']\n",
    "        spk.layer = 0\n",
    "        samna_event_stream.append(spk)\n",
    "\n",
    "    output_events = dynapcnn(samna_event_stream)\n",
    "    neuron_index = [each.feature for each in output_events]\n",
    "    if len(neuron_index) != 0:\n",
    "        frequent_counter = Counter(neuron_index)\n",
    "        prediction = frequent_counter.most_common(1)[0][0]\n",
    "    else:\n",
    "        prediction = -1\n",
    "\n",
    "    if prediction == label:\n",
    "        correct_samples += 1\n",
    "    test_samples += 1\n",
    "\n",
    "print(f\"On chip inference accuracy: {correct_samples / test_samples}\")\n",
    "\n",
    "# 可视化\n",
    "from sinabs.backend.dynapcnn.dynapcnn_visualizer import DynapcnnVisualizer\n",
    "visualizer = DynapcnnVisualizer(window_scale=(4, 8), dvs_shape=(34, 34), spike_collection_interval=50)\n",
    "visualizer.connect(dynapcnn)\n",
    "\n",
    "print(\"Visualizer setup complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
