{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edbed7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from tonic.datasets.nmnist import NMNIST\n",
    "except ImportError:\n",
    "    ! pip install tonic\n",
    "    from tonic.datasets.nmnist import NMNIST\n",
    "    \n",
    "# download dataset\n",
    "root_dir = \"./NMNIST\"\n",
    "_ = NMNIST(save_to=root_dir, train=True)\n",
    "# _ = NMNIST(save_to=root_dir, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64571aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "# define a CNN model\n",
    "cnn = nn.Sequential(\n",
    "    # [2, 34, 34] -> [8, 17, 17]\n",
    "    nn.Conv2d(in_channels=2, out_channels=8, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    # [8, 17, 17] -> [16, 8, 8]\n",
    "    nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    # [16 * 8 * 8] -> [16, 4, 4]\n",
    "    nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=(1, 1), stride=(2, 2),  bias=False),\n",
    "    nn.ReLU(),\n",
    "    # [16 * 4 * 4] -> [10]\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 4 * 4, 10, bias=False),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "# init the model weights\n",
    "for layer in cnn.modules():\n",
    "    if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.xavier_normal_(layer.weight.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "203a2c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transformed array is in shape [Time-Step, Channel, Height, Width] --> (1, 2, 34, 34)\n"
     ]
    }
   ],
   "source": [
    "from tonic.transforms import ToFrame\n",
    "from tonic.datasets import nmnist\n",
    "\n",
    "# define a transform that accumulate the events into a single frame image\n",
    "to_frame = ToFrame(sensor_size=NMNIST.sensor_size, n_time_bins=1)\n",
    "\n",
    "cnn_train_dataset = NMNIST(save_to=root_dir, train=True, transform=to_frame)\n",
    "cnn_test_dataset = NMNIST(save_to=root_dir, train=False, transform=to_frame)\n",
    "\n",
    "# check the transformed data\n",
    "sample_data, label = cnn_train_dataset[0]\n",
    "print(f\"The transformed array is in shape [Time-Step, Channel, Height, Width] --> {sample_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a90ed73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\wangz\\miniconda3\\envs\\cs320\\lib\\site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\wangz\\miniconda3\\envs\\cs320\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\wangz\\miniconda3\\envs\\cs320\\lib\\site-packages (from ipywidgets) (8.21.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\wangz\\miniconda3\\envs\\cs320\\lib\\site-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\wangz\\miniconda3\\envs\\cs320\\lib\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\wangz\\miniconda3\\envs\\cs320\\lib\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in c:\\users\\wangz\\miniconda3\\envs\\cs320\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\wangz\\miniconda3\\envs\\cs320\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\wangz\\miniconda3\\envs\\cs320\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\wangz\\miniconda3\\envs\\cs320\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.42)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\wangz\\miniconda3\\envs\\cs320\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\wangz\\miniconda3\\envs\\cs320\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\wangz\\miniconda3\\envs\\cs320\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\wangz\\miniconda3\\envs\\cs320\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\wangz\\miniconda3\\envs\\cs320\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\wangz\\miniconda3\\envs\\cs320\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\wangz\\miniconda3\\envs\\cs320\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\wangz\\miniconda3\\envs\\cs320\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\wangz\\miniconda3\\envs\\cs320\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\wangz\\miniconda3\\envs\\cs320\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb18e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "epochs = 0\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "shuffle = True\n",
    "\n",
    "cnn = cnn.to(device=device)\n",
    "\n",
    "cnn_train_dataloader = DataLoader(cnn_train_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=shuffle)\n",
    "cnn_test_dataloader = DataLoader(cnn_test_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=shuffle)\n",
    "\n",
    "optimizer = SGD(params=cnn.parameters(), lr=lr)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    # train\n",
    "    train_p_bar = tqdm(cnn_train_dataloader)\n",
    "    for data, label in train_p_bar:\n",
    "        # remove the time-step axis since we are training CNN\n",
    "        # move the data to accelerator\n",
    "        data = data.squeeze(dim=1).to(dtype=torch.float, device=device)\n",
    "        label = label.to(dtype=torch.long, device=device)\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        output = cnn(data)\n",
    "        loss = criterion(output, label)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # set progressing bar\n",
    "        train_p_bar.set_description(f\"Epoch {e} - Training Loss: {round(loss.item(), 4)}\")\n",
    "\n",
    "    # validate\n",
    "    correct_predictions = []\n",
    "    with torch.no_grad():\n",
    "        test_p_bar = tqdm(cnn_test_dataloader)\n",
    "        for data, label in test_p_bar:\n",
    "            # remove the time-step axis since we are training CNN\n",
    "            # move the data to accelerator\n",
    "            data = data.squeeze(dim=1).to(dtype=torch.float, device=device)\n",
    "            label = label.to(dtype=torch.long, device=device)\n",
    "            # forward\n",
    "            output = cnn(data)\n",
    "            # calculate accuracy\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            # compute the total correct predictions\n",
    "            correct_predictions.append(pred.eq(label.view_as(pred)))\n",
    "            # set progressing bar\n",
    "            test_p_bar.set_description(f\"Epoch {e} - Testing Model...\")\n",
    "    \n",
    "        correct_predictions = torch.cat(correct_predictions)\n",
    "        print(f\"Epoch {e} - accuracy: {correct_predictions.sum().item()/(len(correct_predictions))*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2d6d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型到本地文件\n",
    "torch.save(cnn.state_dict(), \"cnn_trained_model.pth\")\n",
    "torch.save(cnn, \"cnn_entire_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6db3b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = torch.load(\"cnn_entire_model.pth\",map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b3658be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=4, num_timesteps=-1)\n",
       "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=4, num_timesteps=-1)\n",
       "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (7): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=4, num_timesteps=-1)\n",
       "  (8): Flatten(start_dim=1, end_dim=-1)\n",
       "  (9): Linear(in_features=256, out_features=10, bias=False)\n",
       "  (10): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=4, num_timesteps=-1)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sinabs.from_torch import from_model\n",
    "\n",
    "snn_convert = from_model(model=cnn, input_shape=(2, 34, 34), batch_size=batch_size).spiking_model\n",
    "snn_convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6eb15b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d9bd21bf6e4f45b6f20a124f1de24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/torch/__init__.py\", line 1854, in <module>\n",
      "    from . import _meta_registrations\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/torch/_meta_registrations.py\", line 9, in <module>\n",
      "    from torch._decomp import (\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/torch/_decomp/__init__.py\", line 244, in <module>\n",
      "    import torch._refs\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/torch/_refs/__init__.py\", line 3084, in <module>\n",
      "    def native_layer_norm(\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/torch/_decomp/__init__.py\", line 185, in decomposition_decorator\n",
      "    pytree.tree_map_(register, aten_op)\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/torch/utils/_pytree.py\", line 607, in tree_map_\n",
      "    deque(map(func, flat_args), maxlen=0)  # consume and exhaust the iterable\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/torch/_decomp/__init__.py\", line 182, in register\n",
      "    _add_op_to_registry(registry, op, fn)\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/torch/_decomp/__init__.py\", line 51, in _add_op_to_registry\n",
      "    overloads.append(getattr(op, ol))\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/torch/_ops.py\", line 737, in __getattr__\n",
      "    overload = OpOverload(self, op_, op_dk_, schema, tags)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     11\u001b[0m     test_p_bar \u001b[38;5;241m=\u001b[39m tqdm(snn_test_dataloader)\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, label \u001b[38;5;129;01min\u001b[39;00m test_p_bar:\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m# reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m34\u001b[39m, \u001b[38;5;241m34\u001b[39m)\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     15\u001b[0m         label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define a transform that accumulate the events into a raster-like tensor\n",
    "n_time_steps = 100\n",
    "to_raster = ToFrame(sensor_size=NMNIST.sensor_size, n_time_bins=n_time_steps)\n",
    "snn_test_dataset = NMNIST(save_to=root_dir, train=False, transform=to_raster)\n",
    "snn_test_dataloader = DataLoader(snn_test_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=False)\n",
    "\n",
    "snn_convert = snn_convert.to(device)\n",
    "\n",
    "correct_predictions = []\n",
    "with torch.no_grad():\n",
    "    test_p_bar = tqdm(snn_test_dataloader)\n",
    "    for data, label in test_p_bar:\n",
    "        # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
    "        data = data.reshape(-1, 2, 34, 34).to(dtype=torch.float, device=device)\n",
    "        label = label.to(dtype=torch.long, device=device)\n",
    "        # forward\n",
    "        output = snn_convert(data)\n",
    "        # reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\n",
    "        output = output.reshape(batch_size, n_time_steps, -1)\n",
    "        # accumulate all time-steps output for final prediction\n",
    "        output = output.sum(dim=1)\n",
    "        # calculate accuracy\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        # compute the total correct predictions\n",
    "        correct_predictions.append(pred.eq(label.view_as(pred)))\n",
    "        # set progressing bar\n",
    "        test_p_bar.set_description(f\"Testing SNN Model...\")\n",
    "\n",
    "    correct_predictions = torch.cat(correct_predictions)\n",
    "    print(f\"accuracy of converted SNN: {correct_predictions.sum().item()/(len(correct_predictions))*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e306dd64",
   "metadata": {},
   "source": [
    "### SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "110a40a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sinabs.layers as sl\n",
    "from torch import nn\n",
    "from sinabs.activation.surrogate_gradient_fn import PeriodicExponential\n",
    "\n",
    "# just replace the ReLU layer with the sl.IAFSqueeze\n",
    "snn_bptt = nn.Sequential(\n",
    "    # [2, 34, 34] -> [8, 17, 17]\n",
    "    nn.Conv2d(in_channels=2, out_channels=8, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
    "    sl.IAFSqueeze(batch_size=batch_size, min_v_mem=-1.0, surrogate_grad_fn=PeriodicExponential()),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    # [8, 17, 17] -> [16, 8, 8]\n",
    "    nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
    "    sl.IAFSqueeze(batch_size=batch_size, min_v_mem=-1.0, surrogate_grad_fn=PeriodicExponential()),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    # [16 * 8 * 8] -> [16, 4, 4]\n",
    "    nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=(1, 1), stride=(2, 2),  bias=False),\n",
    "    sl.IAFSqueeze(batch_size=batch_size, min_v_mem=-1.0, surrogate_grad_fn=PeriodicExponential()),\n",
    "    # [16 * 4 * 4] -> [10]\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 4 * 4, 10, bias=False),\n",
    "    sl.IAFSqueeze(batch_size=batch_size, min_v_mem=-1.0, surrogate_grad_fn=PeriodicExponential()),\n",
    ")\n",
    "\n",
    "# init the model weights\n",
    "for layer in snn_bptt.modules():\n",
    "    if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.xavier_normal_(layer.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7543122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a579657",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_time_steps = 100\n",
    "to_raster = ToFrame(sensor_size=NMNIST.sensor_size, n_time_bins=n_time_steps)\n",
    "\n",
    "snn_train_dataset = NMNIST(save_to=root_dir, train=True, transform=to_raster)\n",
    "snn_test_dataset = NMNIST(save_to=root_dir, train=False, transform=to_raster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2626de58",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 0\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "# device = \"cuda:0\"\n",
    "shuffle = True\n",
    "\n",
    "snn_train_dataloader = DataLoader(snn_train_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=True)\n",
    "snn_test_dataloader = DataLoader(snn_test_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=False)\n",
    "\n",
    "snn_bptt = snn_bptt.to(device=device)\n",
    "\n",
    "optimizer = SGD(params=snn_bptt.parameters(), lr=lr)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    # train\n",
    "    train_p_bar = tqdm(snn_train_dataloader)\n",
    "    for data, label in train_p_bar:\n",
    "        # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
    "        data = data.reshape(-1, 2, 34, 34).to(dtype=torch.float, device=device)\n",
    "        label = label.to(dtype=torch.long, device=device)\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        output = snn_bptt(data)\n",
    "        # reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\n",
    "        output = output.reshape(batch_size, n_time_steps, -1)\n",
    "        # accumulate all time-steps output for final prediction\n",
    "        output = output.sum(dim=1)\n",
    "        loss = criterion(output, label)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # detach the neuron states and activations from current computation graph(necessary)\n",
    "        for layer in snn_bptt.modules():\n",
    "            if isinstance(layer, sl.StatefulLayer):\n",
    "                for name, buffer in layer.named_buffers():\n",
    "                    buffer.detach_()\n",
    "        \n",
    "        # set progressing bar\n",
    "        train_p_bar.set_description(f\"Epoch {e} - BPTT Training Loss: {round(loss.item(), 4)}\")\n",
    "\n",
    "    # validate\n",
    "    correct_predictions = []\n",
    "    with torch.no_grad():\n",
    "        test_p_bar = tqdm(snn_test_dataloader)\n",
    "        for data, label in test_p_bar:\n",
    "            # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
    "            data = data.reshape(-1, 2, 34, 34).to(dtype=torch.float, device=device)\n",
    "            label = label.to(dtype=torch.long, device=device)\n",
    "            # forward\n",
    "            output = snn_bptt(data)\n",
    "            # reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\n",
    "            output = output.reshape(batch_size, n_time_steps, -1)\n",
    "            # accumulate all time-steps output for final prediction\n",
    "            output = output.sum(dim=1)\n",
    "            # calculate accuracy\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            # compute the total correct predictions\n",
    "            correct_predictions.append(pred.eq(label.view_as(pred)))\n",
    "            # set progressing bar\n",
    "            test_p_bar.set_description(f\"Epoch {e} - BPTT Testing Model...\")\n",
    "    \n",
    "        correct_predictions = torch.cat(correct_predictions)\n",
    "        print(f\"Epoch {e} - BPTT accuracy: {correct_predictions.sum().item()/(len(correct_predictions))*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51bdd8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(snn_bptt.state_dict(), \"snn_bptt_model.pth\")\n",
    "torch.save(snn_bptt, \"snn_bptt_fullmodel.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "712ac6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=4, num_timesteps=-1)\n",
       "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=4, num_timesteps=-1)\n",
       "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (7): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=4, num_timesteps=-1)\n",
       "  (8): Flatten(start_dim=1, end_dim=-1)\n",
       "  (9): Linear(in_features=256, out_features=10, bias=False)\n",
       "  (10): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=4, num_timesteps=-1)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snn_bptt = torch.load(\"snn_bptt_fullmodel.pth\",map_location=\"cpu\")\n",
    "snn_bptt.eval().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "160eca0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network is valid\n",
      "The SNN is deployed on the core: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "from sinabs.backend.dynapcnn import DynapcnnNetwork\n",
    "\n",
    "# cpu_snn = snn_convert.to(device=\"cpu\")\n",
    "cpu_snn = snn_bptt.to(device=\"cpu\")\n",
    "dynapcnn = DynapcnnNetwork(snn=cpu_snn, input_shape=(2, 34, 34), discretize=True, dvs_input=False)\n",
    "devkit_name = \"speck2fdevkit\"\n",
    "\n",
    "# use the `to` method of DynapcnnNetwork to deploy the SNN to the devkit\n",
    "dynapcnn.to(device=devkit_name, chip_layers_ordering=\"auto\")\n",
    "print(f\"The SNN is deployed on the core: {dynapcnn.chip_layers_ordering}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55ce160d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0339a1e38b4f4f778d65261ee3be244f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On chip inference accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "import samna\n",
    "from collections import Counter\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "snn_test_dataset = NMNIST(save_to=root_dir, train=False)\n",
    "# for time-saving, we only select a subset for on-chip infernce， here we select 1/100 for an example run\n",
    "subset_indices = list(range(0, len(snn_test_dataset), 100))\n",
    "snn_test_dataset = Subset(snn_test_dataset, subset_indices)\n",
    "\n",
    "inferece_p_bar = tqdm(snn_test_dataset)\n",
    "\n",
    "test_samples = 0\n",
    "correct_samples = 0\n",
    "\n",
    "for events, label in inferece_p_bar:\n",
    "\n",
    "    # create samna Spike events stream\n",
    "    samna_event_stream = []\n",
    "    for ev in events:\n",
    "        spk = samna.speck2f.event.Spike()\n",
    "        spk.x = ev['x']\n",
    "        spk.y = ev['y']\n",
    "        spk.timestamp = ev['t'] - events['t'][0]\n",
    "        spk.feature = ev['p']\n",
    "        # Spikes will be sent to layer/core #0, since the SNN is deployed on core: [0, 1, 2, 3]\n",
    "        spk.layer = 0\n",
    "        samna_event_stream.append(spk)\n",
    "\n",
    "    # inference on chip\n",
    "    # output_events is also a list of Spike, but each Spike.layer is 3, since layer#3 is the output layer\n",
    "    output_events = dynapcnn(samna_event_stream)\n",
    "    \n",
    "    # use the most frequent output neruon index as the final prediction\n",
    "    neuron_index = [each.feature for each in output_events]\n",
    "    if len(neuron_index) != 0:\n",
    "        frequent_counter = Counter(neuron_index)\n",
    "        prediction = frequent_counter.most_common(1)[0][0]\n",
    "    else:\n",
    "        prediction = -1\n",
    "    inferece_p_bar.set_description(f\"label: {label}, prediction: {prediction}， output spikes num: {len(output_events)}\") \n",
    "\n",
    "    if prediction == label:\n",
    "        correct_samples += 1\n",
    "\n",
    "    test_samples += 1\n",
    "    \n",
    "print(f\"On chip inference accuracy: {correct_samples / test_samples}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4907d7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad3653bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network is valid\n",
      "The SNN is deployed on the core: [0, 1, 2, 3]\n",
      "Connecting: Please wait until the JIT compilation is done, this might take a while. You will get notified on completion.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'PyCapsule' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 39\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe SNN is deployed on the core: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdynapcnn\u001b[38;5;241m.\u001b[39mchip_layers_ordering\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m visualizer \u001b[38;5;241m=\u001b[39m DynapcnnVisualizer(\n\u001b[1;32m     34\u001b[0m     window_scale\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m8\u001b[39m),\n\u001b[1;32m     35\u001b[0m     dvs_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m34\u001b[39m, \u001b[38;5;241m34\u001b[39m),\n\u001b[1;32m     36\u001b[0m     spike_collection_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     37\u001b[0m )\n\u001b[0;32m---> 39\u001b[0m \u001b[43mvisualizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdynapcnn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/sinabs/backend/dynapcnn/dynapcnn_visualizer.py:449\u001b[0m, in \u001b[0;36mDynapcnnVisualizer.connect\u001b[0;34m(self, dynapcnn_network, disjoint_process)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreamer_graph \u001b[38;5;241m=\u001b[39m samna\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mEventFilterGraph()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m## Start visualizer and create plots based on parameters.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_visualizer_process\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisualizer_endpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtcp://0.0.0.0:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamna_visualizer_port\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisjoint_process\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisjoint_process\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# Streamer graph\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# Dvs node\u001b[39;00m\n\u001b[1;32m    456\u001b[0m (_, dvs_member_filter, _, streamer_node) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreamer_graph\u001b[38;5;241m.\u001b[39msequential(\n\u001b[1;32m    457\u001b[0m     [\n\u001b[1;32m    458\u001b[0m         dynapcnn_network\u001b[38;5;241m.\u001b[39msamna_device\u001b[38;5;241m.\u001b[39mget_model_source_node(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    462\u001b[0m     ]\n\u001b[1;32m    463\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/sinabs/backend/dynapcnn/dynapcnn_visualizer.py:203\u001b[0m, in \u001b[0;36mDynapcnnVisualizer.create_visualizer_process\u001b[0;34m(self, visualizer_endpoint, disjoint_process)\u001b[0m\n\u001b[1;32m    200\u001b[0m width_proportion \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m16\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_scale[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Create and start the process\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m gui_process \u001b[38;5;241m=\u001b[39m \u001b[43mlaunch_visualizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreceiver_endpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualizer_endpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth_proportion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth_proportion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight_proportion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight_proportion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisjoint_process\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisjoint_process\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gui_process\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/sinabs/backend/dynapcnn/io.py:309\u001b[0m, in \u001b[0;36mlaunch_visualizer\u001b[0;34m(receiver_endpoint, width_proportion, height_proportion, disjoint_process)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     gui_process \u001b[38;5;241m=\u001b[39m Process(\n\u001b[1;32m    306\u001b[0m         target\u001b[38;5;241m=\u001b[39msamnagui\u001b[38;5;241m.\u001b[39mrun_visualizer,\n\u001b[1;32m    307\u001b[0m         args\u001b[38;5;241m=\u001b[39m(receiver_endpoint, width_proportion, height_proportion),\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 309\u001b[0m     \u001b[43mgui_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gui_process\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/multiprocessing/popen_spawn_posix.py:47\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, fp)\n\u001b[0;32m---> 47\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/multiprocessing/reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle 'PyCapsule' object"
     ]
    }
   ],
   "source": [
    "from sinabs.backend.dynapcnn.dynapcnn_visualizer import DynapcnnVisualizer\n",
    "\n",
    "import samna\n",
    "# first define a callback function to modify the devkit configuration\n",
    "# the callback function should only has 1 devkit config instance as its input argument\n",
    "def config_modify_callback(devkit_cfg):\n",
    "\n",
    "    # enable visualizing the output from dvs(pre-processing) layer\n",
    "    devkit_cfg.dvs_layer.monitor_enable = True\n",
    "    # disable visualizing the events generated by the embedded dvs on Speck\n",
    "    devkit_cfg.dvs_layer.raw_monitor_enable = False\n",
    "    # prevent the events generated by the embedded dvs been feed to the DynapCNN Core.\n",
    "    devkit_cfg.dvs_layer.pass_sensor_events = False\n",
    "    # point the dvs layer output destination to the core#0 \n",
    "    devkit_cfg.dvs_layer.destinations[0].enable = True\n",
    "    devkit_cfg.dvs_layer.destinations[0].layer = 0\n",
    "\n",
    "    # the callback must return the modified devkit config\n",
    "    return devkit_cfg\n",
    "\n",
    "# close the devkit before reopen\n",
    "# samna.device.close_device(dynapcnn.samna_device)\n",
    "\n",
    "# init DynapcnnNetwork instance\n",
    "dynapcnn = DynapcnnNetwork(snn=cpu_snn, input_shape=(2, 34, 34), discretize=True, dvs_input=True)\n",
    "\n",
    "# define which layers output you want to monitor\n",
    "layers_to_monitor = [0, 1, 2, 3]\n",
    "# pass the callback function into the `.to` method\n",
    "dynapcnn.to(device=devkit_name, chip_layers_ordering=[0, 1, 2, 3], monitor_layers=layers_to_monitor, config_modifier=config_modify_callback)\n",
    "print(f\"The SNN is deployed on the core: {dynapcnn.chip_layers_ordering}\")\n",
    "\n",
    "visualizer = DynapcnnVisualizer(\n",
    "    window_scale=(4, 8),\n",
    "    dvs_shape=(34, 34),\n",
    "    spike_collection_interval=50,\n",
    ")\n",
    "\n",
    "visualizer.connect(dynapcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b22a192b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1164ce170>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1437, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87359c66a1914b888c2b20e6b079b295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m     samna_event_stream\u001b[38;5;241m.\u001b[39mappend(dvs_ev)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# inference on chip\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# output_events is also a list of Spike, but .layer will have 0, 1, 2, 3 since we choose to monitor all layers' output\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m output_events \u001b[38;5;241m=\u001b[39m \u001b[43mdynapcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamna_event_stream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# get each layers output spikes\u001b[39;00m\n\u001b[1;32m     31\u001b[0m layer0_spks \u001b[38;5;241m=\u001b[39m [each\u001b[38;5;241m.\u001b[39mfeature \u001b[38;5;28;01mfor\u001b[39;00m each \u001b[38;5;129;01min\u001b[39;00m output_events \u001b[38;5;28;01mif\u001b[39;00m each\u001b[38;5;241m.\u001b[39mlayer \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages/sinabs/backend/dynapcnn/dynapcnn_network.py:441\u001b[0m, in \u001b[0;36mDynapcnnNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    438\u001b[0m received_evts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# Wait a minimum time to guarantee the events were played\u001b[39;00m\n\u001b[0;32m--> 441\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Keep recording if more events are being registered\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from torch.utils.data import Subset\n",
    "import samna\n",
    "\n",
    "snn_test_dataset = NMNIST(save_to=root_dir, train=False)\n",
    "# for time-saving, we only select a subset for on-chip infernce， here we select 1/100 for an example run\n",
    "subset_indices = list(range(0, len(snn_test_dataset), 100))\n",
    "snn_test_dataset = Subset(snn_test_dataset, subset_indices)\n",
    "\n",
    "inferece_p_bar = tqdm(snn_test_dataset)\n",
    "\n",
    "for events, label in inferece_p_bar:\n",
    "\n",
    "    # instead of creating Spike and send it to core#0 directly, we now create DvsEvent(for visualization) and send it to the DVS layer\n",
    "    # since in the \"config_modify_callback\" we point the output destination layer of the DVS layer to layer/core #0\n",
    "    # so the DynacnnCore can still receive the same input as before.\n",
    "    samna_event_stream = []\n",
    "    for ev in events:\n",
    "        dvs_ev = samna.speck2f.event.DvsEvent()\n",
    "        dvs_ev.x = ev['x']\n",
    "        dvs_ev.y = ev['y']\n",
    "        dvs_ev.timestamp = ev['t'] - events['t'][0]\n",
    "        dvs_ev.p = ev['p']\n",
    "        samna_event_stream.append(dvs_ev)\n",
    "\n",
    "    # inference on chip\n",
    "    # output_events is also a list of Spike, but .layer will have 0, 1, 2, 3 since we choose to monitor all layers' output\n",
    "    output_events = dynapcnn(samna_event_stream)\n",
    "    \n",
    "    # get each layers output spikes\n",
    "    layer0_spks = [each.feature for each in output_events if each.layer == 0]\n",
    "    layer1_spks = [each.feature for each in output_events if each.layer == 1]\n",
    "    layer2_spks = [each.feature for each in output_events if each.layer == 2]\n",
    "    layer3_spks = [each.feature for each in output_events if each.layer == 3]\n",
    "    # use the most frequent output neruon index as the final prediction\n",
    "    if len(layer3_spks) != 0:\n",
    "        frequent_counter = Counter(layer3_spks)\n",
    "        prediction = frequent_counter.most_common(1)[0][0]\n",
    "    else:\n",
    "        prediction = -1\n",
    "    inferece_p_bar.set_description(f\"label: {label} prediction: {prediction}，layer 0 output spks: {len(layer0_spks)},layer 1 output spikes num: {len(layer1_spks)}, layer 2 output spikes num: {len(layer2_spks)},layer 3 output spikes num: {len(layer3_spks)}\") \n",
    "\n",
    "    if prediction == label:\n",
    "        correct_samples += 1\n",
    "\n",
    "    test_samples += 1\n",
    "    \n",
    "print(f\"On chip inference accuracy: {correct_samples / test_samples}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "977ed9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get device name\n",
    "devices = samna.device.get_all_devices()\n",
    "device_names = [each.device_type_name for each in devices]\n",
    "device_names\n",
    "\n",
    "# open devkit\n",
    "devkit = samna.device.open_device(device_names[0])\n",
    "\n",
    "# get the handle of the stop-watch of the devkit\n",
    "stop_watch = devkit.get_stop_watch()\n",
    "\n",
    "# get the handle of the power monitor of the devkit\n",
    "power_monitor = devkit.get_power_monitor()\n",
    "\n",
    "# create samna node for power reading\n",
    "power_source_node = power_monitor.get_source_node()\n",
    "power_buffer_node = samna.BasicSinkNode_unifirm_modules_events_measurement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d0c8b8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph is destroyed without any start! Graph is required to start manually to work.\n"
     ]
    }
   ],
   "source": [
    "# init graph\n",
    "samna_graph = samna.graph.EventFilterGraph()\n",
    "\n",
    "import time\n",
    "\n",
    "# build graph\n",
    "samna_graph.sequential([power_source_node, power_buffer_node])\n",
    "time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "204f807a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n"
     ]
    }
   ],
   "source": [
    "measure_time = 5.0  # seconds\n",
    "sample_rate = 100.0  # Hz\n",
    "\n",
    "# start samna graph\n",
    "samna_graph.start()\n",
    "\n",
    "# start the stop-watch of devkit, then each output data has a proper timestamp\n",
    "stop_watch.set_enable_value(True)\n",
    "\n",
    "# clear buffer\n",
    "power_buffer_node.get_events()\n",
    "\n",
    "# start monitor, we need pass a sample rate argument to the power monitor\n",
    "power_monitor.start_auto_power_measurement(sample_rate)\n",
    "\n",
    "# sleep the procees to wait for the measurement time up\n",
    "time.sleep(measure_time)\n",
    "\n",
    "# stop monitor\n",
    "power_monitor.stop_auto_power_measurement()\n",
    "\n",
    "# stop samna graph\n",
    "samna_graph.stop()\n",
    "\n",
    "# get power-measurement data\n",
    "power_events = power_buffer_node.get_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "87ab3b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated number of collect data: 2500.0\n",
      "number of collected data: 2499\n"
     ]
    }
   ],
   "source": [
    "# time * sample rate * number of power tracks\n",
    "estimated_number_of_data = measure_time * sample_rate * 5\n",
    "print(f\"estimated number of collect data: {estimated_number_of_data}\")\n",
    "\n",
    "print(f\"number of collected data: {len(power_events)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ccca5af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track0: 40.050064086914084uW, 16.020025634765634uA\n",
      "track1: 76.22732979910695uW, 63.52277483258913uA\n",
      "track2: 323.05977957589295uW, 269.2164829799108uA\n",
      "track3: 33.58468191964272uW, 27.9872349330356uA\n",
      "track4: 737.4028889251692uW, 614.5024074376411uA\n"
     ]
    }
   ],
   "source": [
    "num_power_tracks = 5\n",
    "\n",
    "# init dict for storing data of each power track\n",
    "power_each_track = dict()\n",
    "event_count_each_track = dict()\n",
    "\n",
    "# check whether timestamp is correct\n",
    "timestamp_all_zero = True\n",
    "\n",
    "# loop through all collected power events and get data\n",
    "for evt in power_events:\n",
    "\n",
    "    if evt.timestamp != 0:\n",
    "        timestamp_all_zero = False\n",
    "    \n",
    "    p_track_id = evt.channel\n",
    "    tmp_power = power_each_track.get(p_track_id, 0) + evt.value\n",
    "    tmp_count = event_count_each_track.get(p_track_id, 0) + 1\n",
    "    \n",
    "    power_each_track.update({p_track_id: tmp_power})\n",
    "    event_count_each_track.update({p_track_id: tmp_count})\n",
    "\n",
    "# average power and current of each track\n",
    "for p_track_id in range(num_power_tracks):\n",
    "    \n",
    "    # average power in microwatt\n",
    "    avg_power = power_each_track[p_track_id] / event_count_each_track[p_track_id] * 1e6\n",
    "    # calculate current\n",
    "    if p_track_id == 0:\n",
    "        current = avg_power / 2.5 \n",
    "    else:\n",
    "        current = avg_power / 1.2\n",
    "        \n",
    "    print(f'track{p_track_id}: {avg_power}uW, {current}uA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b54798f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timestamp_all_zero:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamps are all zeros, can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt plot power vs. time, you might need to update the firmware!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "if timestamp_all_zero:\n",
    "    \n",
    "    print(\"timestamps are all zeros, can't plot power vs. time, you might need to update the firmware!\")\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # plot the output neuron index vs. time\n",
    "    plt.style.use('ggplot')\n",
    "    fig, ax = plt.subplots()\n",
    "    p_track_name = [\"io\", \"ram\", \"logic\", \"pixel digital\", \"pixel analog\"]\n",
    "    for p_track_id in range(num_power_tracks):\n",
    "        x = [each.timestamp for each in power_events if each.channel == p_track_id]\n",
    "        y = [each.value * 1e6 for each in power_events if each.channel == p_track_id]\n",
    "        plt.plot(x, y, label=p_track_name[p_track_id], alpha=0.8)\n",
    "        \n",
    "    ax.set_xlabel(\"time(us)\")\n",
    "    ax.set_ylabel(\"power(uW)\")\n",
    "    ax.set_title(\"Idle Power\")\n",
    "    ax.legend(loc=\"upper right\", fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f574ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def measure_average_power(sample_rate=100.0, duration=5.0):\n",
    "    power_monitor.start_auto_power_measurement(sample_rate)\n",
    "    stop_watch.set_enable_value(True)\n",
    "    power_buffer_node.get_events()\n",
    "    time.sleep(duration)\n",
    "    power_monitor.stop_auto_power_measurement()\n",
    "    power_events = power_buffer_node.get_events()\n",
    "\n",
    "    num_tracks = 5\n",
    "    power_sum = [0.0] * num_tracks\n",
    "    power_count = [0] * num_tracks\n",
    "    for evt in power_events:\n",
    "        power_sum[evt.channel] += evt.value\n",
    "        power_count[evt.channel] += 1\n",
    "    avg_uw = [power_sum[i] / power_count[i] * 1e6 for i in range(num_tracks)]\n",
    "    total_mw = sum(avg_uw) / 1000.0\n",
    "    return avg_uw, total_mw\n",
    "\n",
    "# ====== Step 1: Idle Power ======\n",
    "print(\"Measuring idle power...\")\n",
    "idle_avg_uw, idle_total_mw = measure_average_power(duration=5.0)\n",
    "print(f\"Idle power: {idle_total_mw:.2f} mW ({idle_avg_uw})\")\n",
    "\n",
    "# ====== Step 2: Active Power + Inference ======\n",
    "print(\"Running inference and measuring active power...\")\n",
    "power_monitor.start_auto_power_measurement(100.0)\n",
    "stop_watch.set_enable_value(True)\n",
    "power_buffer_node.get_events()\n",
    "start_time = time.time()\n",
    "\n",
    "# N-MNIST 推理\n",
    "snn_test_dataset = NMNIST(save_to=root_dir, train=False)\n",
    "subset_indices = list(range(0, len(snn_test_dataset), 100))\n",
    "snn_test_dataset = Subset(snn_test_dataset, subset_indices)\n",
    "\n",
    "correct_samples = 0\n",
    "test_samples = 0\n",
    "for events, label in tqdm(snn_test_dataset):\n",
    "    spike_events = []\n",
    "    for ev in events:\n",
    "        spk = samna.speck2f.event.Spike()\n",
    "        spk.x, spk.y, spk.timestamp, spk.feature = ev[\"x\"], ev[\"y\"], ev[\"t\"] - events[\"t\"][0], ev[\"p\"]\n",
    "        spk.layer = 0\n",
    "        spike_events.append(spk)\n",
    "\n",
    "    output_events = dynapcnn(spike_events)\n",
    "    layer3_spks = [e.feature for e in output_events if e.layer == 3]\n",
    "    prediction = Counter(layer3_spks).most_common(1)[0][0] if layer3_spks else -1\n",
    "    if prediction == label:\n",
    "        correct_samples += 1\n",
    "    test_samples += 1\n",
    "\n",
    "end_time = time.time()\n",
    "power_monitor.stop_auto_power_measurement()\n",
    "active_power_events = power_buffer_node.get_events()\n",
    "\n",
    "# ====== Step 3: Compute active power ======\n",
    "active_avg_uw = [0.0] * 5\n",
    "active_count = [0] * 5\n",
    "for evt in active_power_events:\n",
    "    active_avg_uw[evt.channel] += evt.value\n",
    "    active_count[evt.channel] += 1\n",
    "active_avg_uw = [active_avg_uw[i] / active_count[i] * 1e6 for i in range(5)]\n",
    "active_total_mw = sum(active_avg_uw) / 1000.0\n",
    "\n",
    "# ====== Step 4: Compute dynamic power & efficiency ======\n",
    "dynamic_mw = active_total_mw - idle_total_mw\n",
    "elapsed = end_time - start_time\n",
    "fps = test_samples / elapsed\n",
    "fps_per_watt = fps / (dynamic_mw / 1000.0) if dynamic_mw > 0 else float('inf')\n",
    "\n",
    "# ====== 输出结果 ======\n",
    "print(f\"\\nOn-chip accuracy: {correct_samples / test_samples:.2%}\")\n",
    "print(f\"Idle power:   {idle_total_mw:.2f} mW\")\n",
    "print(f\"Active power: {active_total_mw:.2f} mW\")\n",
    "print(f\"Dynamic power: {dynamic_mw:.2f} mW\")\n",
    "print(f\"FPS: {fps:.2f}\")\n",
    "print(f\"FPS/W (dynamic): {fps_per_watt:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs320",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
