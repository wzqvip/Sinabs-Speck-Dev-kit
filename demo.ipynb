{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edbed7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from tonic.datasets.nmnist import NMNIST\n",
    "except ImportError:\n",
    "    ! pip install tonic\n",
    "    from tonic.datasets.nmnist import NMNIST\n",
    "    \n",
    "# download dataset\n",
    "root_dir = \"./NMNIST\"\n",
    "_ = NMNIST(save_to=root_dir, train=True)\n",
    "# _ = NMNIST(save_to=root_dir, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64571aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "# define a CNN model\n",
    "cnn = nn.Sequential(\n",
    "    # [2, 34, 34] -> [8, 17, 17]\n",
    "    nn.Conv2d(in_channels=2, out_channels=8, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    # [8, 17, 17] -> [16, 8, 8]\n",
    "    nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    # [16 * 8 * 8] -> [16, 4, 4]\n",
    "    nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=(1, 1), stride=(2, 2),  bias=False),\n",
    "    nn.ReLU(),\n",
    "    # [16 * 4 * 4] -> [10]\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 4 * 4, 10, bias=False),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "# init the model weights\n",
    "for layer in cnn.modules():\n",
    "    if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.xavier_normal_(layer.weight.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "203a2c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transformed array is in shape [Time-Step, Channel, Height, Width] --> (1, 2, 34, 34)\n"
     ]
    }
   ],
   "source": [
    "from tonic.transforms import ToFrame\n",
    "from tonic.datasets import nmnist\n",
    "\n",
    "# define a transform that accumulate the events into a single frame image\n",
    "to_frame = ToFrame(sensor_size=NMNIST.sensor_size, n_time_bins=1)\n",
    "\n",
    "cnn_train_dataset = NMNIST(save_to=root_dir, train=True, transform=to_frame)\n",
    "cnn_test_dataset = NMNIST(save_to=root_dir, train=False, transform=to_frame)\n",
    "\n",
    "# check the transformed data\n",
    "sample_data, label = cnn_train_dataset[0]\n",
    "print(f\"The transformed array is in shape [Time-Step, Channel, Height, Width] --> {sample_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a90ed73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages (8.1.6)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages (from ipywidgets) (8.35.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.14 in /usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages (from ipywidgets) (3.0.14)\n",
      "Requirement already satisfied: decorator in /usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.13.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /usr/local/Caskroom/miniconda/base/envs/py310/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb18e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "epochs = 0\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "shuffle = True\n",
    "\n",
    "cnn = cnn.to(device=device)\n",
    "\n",
    "cnn_train_dataloader = DataLoader(cnn_train_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=shuffle)\n",
    "cnn_test_dataloader = DataLoader(cnn_test_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=shuffle)\n",
    "\n",
    "optimizer = SGD(params=cnn.parameters(), lr=lr)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    # train\n",
    "    train_p_bar = tqdm(cnn_train_dataloader)\n",
    "    for data, label in train_p_bar:\n",
    "        # remove the time-step axis since we are training CNN\n",
    "        # move the data to accelerator\n",
    "        data = data.squeeze(dim=1).to(dtype=torch.float, device=device)\n",
    "        label = label.to(dtype=torch.long, device=device)\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        output = cnn(data)\n",
    "        loss = criterion(output, label)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # set progressing bar\n",
    "        train_p_bar.set_description(f\"Epoch {e} - Training Loss: {round(loss.item(), 4)}\")\n",
    "\n",
    "    # validate\n",
    "    correct_predictions = []\n",
    "    with torch.no_grad():\n",
    "        test_p_bar = tqdm(cnn_test_dataloader)\n",
    "        for data, label in test_p_bar:\n",
    "            # remove the time-step axis since we are training CNN\n",
    "            # move the data to accelerator\n",
    "            data = data.squeeze(dim=1).to(dtype=torch.float, device=device)\n",
    "            label = label.to(dtype=torch.long, device=device)\n",
    "            # forward\n",
    "            output = cnn(data)\n",
    "            # calculate accuracy\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            # compute the total correct predictions\n",
    "            correct_predictions.append(pred.eq(label.view_as(pred)))\n",
    "            # set progressing bar\n",
    "            test_p_bar.set_description(f\"Epoch {e} - Testing Model...\")\n",
    "    \n",
    "        correct_predictions = torch.cat(correct_predictions)\n",
    "        print(f\"Epoch {e} - accuracy: {correct_predictions.sum().item()/(len(correct_predictions))*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b3658be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=4, num_timesteps=-1)\n",
       "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=4, num_timesteps=-1)\n",
       "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (7): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=4, num_timesteps=-1)\n",
       "  (8): Flatten(start_dim=1, end_dim=-1)\n",
       "  (9): Linear(in_features=256, out_features=10, bias=False)\n",
       "  (10): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=4, num_timesteps=-1)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sinabs.from_torch import from_model\n",
    "\n",
    "snn_convert = from_model(model=cnn, input_shape=(2, 34, 34), batch_size=batch_size).spiking_model\n",
    "snn_convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eb15b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98bc2d34314540439e01c1abb1669e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of converted SNN: 9.13%\n"
     ]
    }
   ],
   "source": [
    "# define a transform that accumulate the events into a raster-like tensor\n",
    "n_time_steps = 100\n",
    "to_raster = ToFrame(sensor_size=NMNIST.sensor_size, n_time_bins=n_time_steps)\n",
    "snn_test_dataset = NMNIST(save_to=root_dir, train=False, transform=to_raster)\n",
    "snn_test_dataloader = DataLoader(snn_test_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=False)\n",
    "\n",
    "snn_convert = snn_convert.to(device)\n",
    "\n",
    "correct_predictions = []\n",
    "with torch.no_grad():\n",
    "    test_p_bar = tqdm(snn_test_dataloader)\n",
    "    for data, label in test_p_bar:\n",
    "        # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
    "        data = data.reshape(-1, 2, 34, 34).to(dtype=torch.float, device=device)\n",
    "        label = label.to(dtype=torch.long, device=device)\n",
    "        # forward\n",
    "        output = snn_convert(data)\n",
    "        # reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\n",
    "        output = output.reshape(batch_size, n_time_steps, -1)\n",
    "        # accumulate all time-steps output for final prediction\n",
    "        output = output.sum(dim=1)\n",
    "        # calculate accuracy\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        # compute the total correct predictions\n",
    "        correct_predictions.append(pred.eq(label.view_as(pred)))\n",
    "        # set progressing bar\n",
    "        test_p_bar.set_description(f\"Testing SNN Model...\")\n",
    "\n",
    "    correct_predictions = torch.cat(correct_predictions)\n",
    "    print(f\"accuracy of converted SNN: {correct_predictions.sum().item()/(len(correct_predictions))*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e306dd64",
   "metadata": {},
   "source": [
    "### SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "110a40a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sinabs.layers as sl\n",
    "from torch import nn\n",
    "from sinabs.activation.surrogate_gradient_fn import PeriodicExponential\n",
    "\n",
    "# just replace the ReLU layer with the sl.IAFSqueeze\n",
    "snn_bptt = nn.Sequential(\n",
    "    # [2, 34, 34] -> [8, 17, 17]\n",
    "    nn.Conv2d(in_channels=2, out_channels=8, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
    "    sl.IAFSqueeze(batch_size=batch_size, min_v_mem=-1.0, surrogate_grad_fn=PeriodicExponential()),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    # [8, 17, 17] -> [16, 8, 8]\n",
    "    nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
    "    sl.IAFSqueeze(batch_size=batch_size, min_v_mem=-1.0, surrogate_grad_fn=PeriodicExponential()),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    # [16 * 8 * 8] -> [16, 4, 4]\n",
    "    nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=(1, 1), stride=(2, 2),  bias=False),\n",
    "    sl.IAFSqueeze(batch_size=batch_size, min_v_mem=-1.0, surrogate_grad_fn=PeriodicExponential()),\n",
    "    # [16 * 4 * 4] -> [10]\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 4 * 4, 10, bias=False),\n",
    "    sl.IAFSqueeze(batch_size=batch_size, min_v_mem=-1.0, surrogate_grad_fn=PeriodicExponential()),\n",
    ")\n",
    "\n",
    "# init the model weights\n",
    "for layer in snn_bptt.modules():\n",
    "    if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.xavier_normal_(layer.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7543122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a579657",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_time_steps = 100\n",
    "to_raster = ToFrame(sensor_size=NMNIST.sensor_size, n_time_bins=n_time_steps)\n",
    "\n",
    "snn_train_dataset = NMNIST(save_to=root_dir, train=True, transform=to_raster)\n",
    "snn_test_dataset = NMNIST(save_to=root_dir, train=False, transform=to_raster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2626de58",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "# device = \"cuda:0\"\n",
    "shuffle = True\n",
    "\n",
    "snn_train_dataloader = DataLoader(snn_train_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=True)\n",
    "snn_test_dataloader = DataLoader(snn_test_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=False)\n",
    "\n",
    "snn_bptt = snn_bptt.to(device=device)\n",
    "\n",
    "optimizer = SGD(params=snn_bptt.parameters(), lr=lr)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    # train\n",
    "    train_p_bar = tqdm(snn_train_dataloader)\n",
    "    for data, label in train_p_bar:\n",
    "        # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
    "        data = data.reshape(-1, 2, 34, 34).to(dtype=torch.float, device=device)\n",
    "        label = label.to(dtype=torch.long, device=device)\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        output = snn_bptt(data)\n",
    "        # reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\n",
    "        output = output.reshape(batch_size, n_time_steps, -1)\n",
    "        # accumulate all time-steps output for final prediction\n",
    "        output = output.sum(dim=1)\n",
    "        loss = criterion(output, label)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # detach the neuron states and activations from current computation graph(necessary)\n",
    "        for layer in snn_bptt.modules():\n",
    "            if isinstance(layer, sl.StatefulLayer):\n",
    "                for name, buffer in layer.named_buffers():\n",
    "                    buffer.detach_()\n",
    "        \n",
    "        # set progressing bar\n",
    "        train_p_bar.set_description(f\"Epoch {e} - BPTT Training Loss: {round(loss.item(), 4)}\")\n",
    "\n",
    "    # validate\n",
    "    correct_predictions = []\n",
    "    with torch.no_grad():\n",
    "        test_p_bar = tqdm(snn_test_dataloader)\n",
    "        for data, label in test_p_bar:\n",
    "            # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
    "            data = data.reshape(-1, 2, 34, 34).to(dtype=torch.float, device=device)\n",
    "            label = label.to(dtype=torch.long, device=device)\n",
    "            # forward\n",
    "            output = snn_bptt(data)\n",
    "            # reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\n",
    "            output = output.reshape(batch_size, n_time_steps, -1)\n",
    "            # accumulate all time-steps output for final prediction\n",
    "            output = output.sum(dim=1)\n",
    "            # calculate accuracy\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            # compute the total correct predictions\n",
    "            correct_predictions.append(pred.eq(label.view_as(pred)))\n",
    "            # set progressing bar\n",
    "            test_p_bar.set_description(f\"Epoch {e} - BPTT Testing Model...\")\n",
    "    \n",
    "        correct_predictions = torch.cat(correct_predictions)\n",
    "        print(f\"Epoch {e} - BPTT accuracy: {correct_predictions.sum().item()/(len(correct_predictions))*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bdd8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "160eca0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network is valid\n",
      "The SNN is deployed on the core: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "from sinabs.backend.dynapcnn import DynapcnnNetwork\n",
    "\n",
    "# cpu_snn = snn_convert.to(device=\"cpu\")\n",
    "cpu_snn = snn_bptt.to(device=\"cpu\")\n",
    "dynapcnn = DynapcnnNetwork(snn=cpu_snn, input_shape=(2, 34, 34), discretize=True, dvs_input=False)\n",
    "devkit_name = \"speck2fdevkit\"\n",
    "\n",
    "# use the `to` method of DynapcnnNetwork to deploy the SNN to the devkit\n",
    "dynapcnn.to(device=devkit_name, chip_layers_ordering=\"auto\")\n",
    "print(f\"The SNN is deployed on the core: {dynapcnn.chip_layers_ordering}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55ce160d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e6c969e8574994a932cc2e0d5cf5f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n",
      "`set_enable_value` is DEPRECATED, use `start` and `stop` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On chip inference accuracy: 0.1\n"
     ]
    }
   ],
   "source": [
    "import samna\n",
    "from collections import Counter\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "snn_test_dataset = NMNIST(save_to=root_dir, train=False)\n",
    "# for time-saving, we only select a subset for on-chip infernce， here we select 1/100 for an example run\n",
    "subset_indices = list(range(0, len(snn_test_dataset), 100))\n",
    "snn_test_dataset = Subset(snn_test_dataset, subset_indices)\n",
    "\n",
    "inferece_p_bar = tqdm(snn_test_dataset)\n",
    "\n",
    "test_samples = 0\n",
    "correct_samples = 0\n",
    "\n",
    "for events, label in inferece_p_bar:\n",
    "\n",
    "    # create samna Spike events stream\n",
    "    samna_event_stream = []\n",
    "    for ev in events:\n",
    "        spk = samna.speck2f.event.Spike()\n",
    "        spk.x = ev['x']\n",
    "        spk.y = ev['y']\n",
    "        spk.timestamp = ev['t'] - events['t'][0]\n",
    "        spk.feature = ev['p']\n",
    "        # Spikes will be sent to layer/core #0, since the SNN is deployed on core: [0, 1, 2, 3]\n",
    "        spk.layer = 0\n",
    "        samna_event_stream.append(spk)\n",
    "\n",
    "    # inference on chip\n",
    "    # output_events is also a list of Spike, but each Spike.layer is 3, since layer#3 is the output layer\n",
    "    output_events = dynapcnn(samna_event_stream)\n",
    "    \n",
    "    # use the most frequent output neruon index as the final prediction\n",
    "    neuron_index = [each.feature for each in output_events]\n",
    "    if len(neuron_index) != 0:\n",
    "        frequent_counter = Counter(neuron_index)\n",
    "        prediction = frequent_counter.most_common(1)[0][0]\n",
    "    else:\n",
    "        prediction = -1\n",
    "    inferece_p_bar.set_description(f\"label: {label}, prediction: {prediction}， output spikes num: {len(output_events)}\") \n",
    "\n",
    "    if prediction == label:\n",
    "        correct_samples += 1\n",
    "\n",
    "    test_samples += 1\n",
    "    \n",
    "print(f\"On chip inference accuracy: {correct_samples / test_samples}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3653bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sinabs.backend.dynapcnn.dynapcnn_visualizer import DynapcnnVisualizer\n",
    "\n",
    "\n",
    "visualizer = DynapcnnVisualizer(\n",
    "    window_scale=(4, 8),\n",
    "    dvs_shape=(34, 34),\n",
    "    spike_collection_interval=50,\n",
    ")\n",
    "\n",
    "visualizer.connect(dynapcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22a192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "snn_test_dataset = NMNIST(save_to=root_dir, train=False)\n",
    "# for time-saving, we only select a subset for on-chip infernce， here we select 1/100 for an example run\n",
    "subset_indices = list(range(0, len(snn_test_dataset), 100))\n",
    "snn_test_dataset = Subset(snn_test_dataset, subset_indices)\n",
    "\n",
    "inferece_p_bar = tqdm(snn_test_dataset)\n",
    "\n",
    "for events, label in inferece_p_bar:\n",
    "\n",
    "    # instead of creating Spike and send it to core#0 directly, we now create DvsEvent(for visualization) and send it to the DVS layer\n",
    "    # since in the \"config_modify_callback\" we point the output destination layer of the DVS layer to layer/core #0\n",
    "    # so the DynacnnCore can still receive the same input as before.\n",
    "    samna_event_stream = []\n",
    "    for ev in events:\n",
    "        dvs_ev = samna.speck2f.event.DvsEvent()\n",
    "        dvs_ev.x = ev['x']\n",
    "        dvs_ev.y = ev['y']\n",
    "        dvs_ev.timestamp = ev['t'] - events['t'][0]\n",
    "        dvs_ev.p = ev['p']\n",
    "        samna_event_stream.append(dvs_ev)\n",
    "\n",
    "    # inference on chip\n",
    "    # output_events is also a list of Spike, but .layer will have 0, 1, 2, 3 since we choose to monitor all layers' output\n",
    "    output_events = dynapcnn(samna_event_stream)\n",
    "    \n",
    "    # get each layers output spikes\n",
    "    layer0_spks = [each.feature for each in output_events if each.layer == 0]\n",
    "    layer1_spks = [each.feature for each in output_events if each.layer == 1]\n",
    "    layer2_spks = [each.feature for each in output_events if each.layer == 2]\n",
    "    layer3_spks = [each.feature for each in output_events if each.layer == 3]\n",
    "    # use the most frequent output neruon index as the final prediction\n",
    "    if len(layer3_spks) != 0:\n",
    "        frequent_counter = Counter(layer3_spks)\n",
    "        prediction = frequent_counter.most_common(1)[0][0]\n",
    "    else:\n",
    "        prediction = -1\n",
    "    inferece_p_bar.set_description(f\"label: {label} prediction: {prediction}，layer 0 output spks: {len(layer0_spks)},layer 1 output spikes num: {len(layer1_spks)}, layer 2 output spikes num: {len(layer2_spks)},layer 3 output spikes num: {len(layer3_spks)}\") \n",
    "\n",
    "    if prediction == label:\n",
    "        correct_samples += 1\n",
    "\n",
    "    test_samples += 1\n",
    "    \n",
    "print(f\"On chip inference accuracy: {correct_samples / test_samples}\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
